# 实时音视频人像克隆技术开发笔记

Real Time Portrait Cloning

# 1、调研

## 1.1、人脸克隆相关开源框架

### 实时视频换脸工具 Avatarify

Avatarify 是一款基于 [first-order-model](https://github.com/AliaksandrSiarohin/first-order-model) 的实时变脸软件，可以在 ZOOM、skype 等视频会议软件中使用，将名人的脸套在自己的脸上。项目作者之一 Ali Aliev 介绍说，Avatarify 使用了今年三月一篇 arXiv 论文的方法「First Order Motion」，无需事先对目标图像进行任何训练，就能用另一个人的视频来替换自己的图像。

文章介绍：

[用实时换脸AI，跟特朗普、爱因斯坦Zoom连线](https://www.huxiu.com/article/352484.html)

[和马斯克Zoom开个会，竟是AI换脸，GitHub 4000星项目登上热榜](https://zhuanlan.zhihu.com/p/135321992)

Github开源代码：

https://github.com/alievk/avatarify

Demo教程：

项目在 avatars 目录下预先储存了一些名人的头像，当然我们也可以把它换成我们自己喜欢的目标头像。为了达到较好的换脸效果，需要注意将目标头像裁剪为正方形。同时，以下图作为参考，目标头像的距离不能太远也不能太近。最好选择单一的背景，以最大程度上避免还原失真。

Avatarify 目前还无法模仿声音，这件事你得自己来。不过已经有一些实时语音克隆工具面世了，「[Real-Time Voice Cloning](https://link.zhihu.com/?target=https%3A//github.com/CorentinJ/Real-Time-Voice-Cloning)」这个项目最近也登上了 GitHub 热榜。如果两者完美结合，有很多大胆的想法应该就可以实现了……

### Real time Image Animation

The Project is real time application in opencv using first order model

Github开源代码：

https://github.com/anandpawara/Real_Time_Image_Animation

### First Order Motion Model for Image Animation

https://github.com/AliaksandrSiarohin/first-order-model

https://github.com/AliaksandrSiarohin/motion-cosegmentation

https://github.com/AliaksandrSiarohin/pose-evaluation

https://github.com/AliaksandrSiarohin/video-preprocessing

https://github.com/1adrianb/face-alignment

### Face2Face升级版HeadOn

“变脸”技术已经不新奇，来自德国慕尼黑工业大学、斯坦福大学等的一组研究人员最近开发了一个叫“HeadOn”的AI，它可以“变人”——根据输入人物的动作，实时地改变视频中人物的面部表情、眼球运动和身体动作，使得图像中的人看起来像是真的在说话和移动一样。

文章介绍：

[【换脸 AI 升级版】面部表情、身体动作、视线方向都能实时迁移](https://bbs.cvmart.net/topics/807)

[忘了DeepFakes吧，Deep Video Portraits的“换脸术”好到可怕](https://cloud.tencent.com/developer/news/235189)

[一款开源换脸工具FaceSwap以超过23000星登上GitHub排行榜](http://www.elecfans.com/d/1065647.html)

[ZAO有风险！开源换脸工具FakeSwap今登GitHub排行榜，你也可以玩！](https://cloud.tencent.com/developer/article/1498395)



> # 换脸技术发展简史
>
> 概述：换脸技术的应用分为三类，即换脸/换五官，转换面部表情，转换面部与躯干运动。以下介绍按照时间顺序排列。
>
> ### 1. Face2Face：Real-time Face Capture and Reenactment of RGB Videos（转换面部表情）
>
> 由德国纽伦堡大学科学家 Justus Thies 的团队在 CVPR 2016 发布
>
> 可以非常逼真的将一个人的面部表情、说话时面部肌肉的变化、嘴型等完美地实时复制到另一个人脸上
>
> 论文：[http://www.graphics.stanford.edu/~niessner/papers/2016/1facetoface/thies2016face.pdf](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.graphics.stanford.edu%2F~niessner%2Fpapers%2F2016%2F1facetoface%2Fthies2016face.pdf)
>
> ### 2. Deepfake：Face Swapping（换脸 / 换五官）
>
> 推出了 FakeApp（适用于Windows）
>
> Github 上有相同的开源项目 FaceSwap（适用于Ubuntu） [https://github.com/deepfakes/faceswap](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fdeepfakes%2Ffaceswap)
>
> ### 3. HeadOn：Real-time Reenactment of Human Portrait Videos（转换面部表情与躯干运动）
>
> 由 Face2Face 原班团队做的升级版，在 ACM Transactions on Graphics 2018 发布
>
> Face2Face 只能实现面部表情的转换，HeadOn 增加了身体运动和头部运动的迁移
>
> 论文：[ http://niessnerlab.org/papers/2018/7headon/headon_preprint.pdf](https://links.jianshu.com/go?to=http%3A%2F%2Fniessnerlab.org%2Fpapers%2F2018%2F7headon%2Fheadon_preprint.pdf)
>
> ### 4. FSGAN：Subject Agnostic Face Swapping and Reenactment （换脸 & 转换面部表情）
>
> ICCV 2019 未开源
>
> 论文：[https://arxiv.org/pdf/1908.05932.pdf](https://links.jianshu.com/go?to=https%3A%2F%2Farxiv.org%2Fpdf%2F1908.05932.pdf)
>
> 
>
> 作者：牛牪犇_9c01
> 链接：https://www.jianshu.com/p/67eb20c3420e



## 1.2、语音克隆相关开源框架

### [Real-Time-Voice-Cloning](https://github.com/CorentinJ/Real-Time-Voice-Cloning)（TTS）

Clone a voice in 5 seconds to generate arbitrary speech in real-time.

Project here: https://github.com/CorentinJ/Real-Time-Voice-Cloning

Original paper: [https://arxiv.org/abs/1806.04558](https://www.youtube.com/redirect?redir_token=QUFFLUhqa0VkdjQySDJrYUU5RlJ6aWtEX2ZjZE42LUlJZ3xBQ3Jtc0tuTWxUVmxCY3dkMFNXWmtZUldEMGhpREg0YmhadjI0NF91R3YzZGNoRnhBN2UtTnRpa1diSjdLRzhqZEpJbVZGd0RfTWVqODQxbngtZnpyQVBaRHphR3JRQy1ieEUxOFBQSFRCeHphVFVCQVhhUmNCMA%3D%3D&q=https%3A%2F%2Farxiv.org%2Fabs%2F1806.04558&event=video_description&v=-O_hYhToKoA)

### [TTS-Clone-Chinese](https://github.com/aidreamwin/TTS-Clone-Chinese)（TTS）

基于Real-Time-Voice-Cloning语音克隆中文普通话实现。

https://github.com/aidreamwin/TTS-Clone-Chinese

### Tacotron（TTS）

[Tacotron 2 (without wavenet)](https://github.com/NVIDIA/tacotron2)

[Tacotron](https://github.com/keithito/tacotron)

### TensorFlowTTS（TTS）

https://github.com/TensorSpeech/TensorFlowTTS



### Voice-Conversion（multi-speaker voice conversion）

[StarGAN-Voice-Conversion](https://github.com/dipjyoti92/StarGAN-Voice-Conversion)

[pytorch-StarGAN-VC2](https://github.com/Oscarshu0719/pytorch-StarGAN-VC2)



语音转换研究的相关工作最早可追溯至20世纪六七十年代，至今已经有50多年的研究历史，但真正受到学术界和产业界广泛关注则是近十多年的事情。近年来，语音信号处理和机器学习等技术的进步，以及大数据获取能力和大规模计算性能的提高有力地推动了语音转换技术的研究及发展。

特别是基于人工神经网络(Artificial neural network, ANN)的语音转换方法的兴起，使得转换语音的质量进一步得到提升。国内较早进行语音转换研究的机构包括中国科学院、中国科学技术大学、国防科技大学、亚洲微软研究院、IBM中国研究院等。

近年来，东南大学、南京邮电大学、华南理工大学、苏州大学、哈尔滨工业大学、西北工业大学、陆军工程大学等多所高校以及腾讯、科大讯飞和百度等多家企业也开始此项技术研究，并相继取得了一些的研究成果。

2016年，来自中、日、英等国语音转换领域的科学家组织了VCC2016语音转换竞赛，在统一的数据集上，对17个国际著名的语音研究小组提交的系统做了统一的评价和分析，为语音转换研究提供了数据平台和性能标尺。

2018年VCC2018也如期举办，语音转换方法再次推陈出新，且转换语音的质量也得到明显提升。



> # 音频中的风格迁移
>
> 本文中所谓的风格迁移是一种非常宽泛的说法，包括论文中所谓的“语音克隆”(voice clone)、“多说话人风格迁移”(multi-speaker)、“风格迁移”(style transfer)、“语音转换”(voice conversion)。
>
> ### Voice Conversion
>
> 博客地址：[VOICE CONVERSION](https://www.andrewszot.com/blog/machine_learning/deep_learning/voice_conversion)
>
> 本文综述了2017年及之前出现语音风格迁移的方法。文中提到，深度网络对于语音处理困难。一来不如图像和文本领域研究火热，二来语音所具有的信息，难以编码到高维隐空间。一段语音时间序列中，混杂着以下方面的信息：说话人特征(如一个人的音色，音调等)；语言学内容(语音表述的内容)；副语言特征(如情感等)。
>
> 文中提供了4种方法：
>
> - 直接对频谱图卷积。利用现有的图像风格迁移的方法，直接对频谱图卷积。但是这种方法不能提取语音中的局部信息，更善于识别音频中的全局重复频率。文中使用CycleGAN实现了该方法。
>
>   github上利用该思想实现的语音风格迁移：[mazzzystar/randomCNN-voice-transfer](https://github.com/mazzzystar/randomCNN-voice-transfer)
>
>   实验中，该方法生成的语音质量很差，风格迁移效果不明显，生成速度过慢。
>
> - Supervised latent space / Unconditional generator.
>
> - Supervised latent space / Speaker conditioned genenrator.
>
> - Unsupervised latent space / Speaker conditioned generator. 文中使用VQ-VAE实现。
>
> 文中使用的数据集：
>
> - [CSTR VCTK Corpus](http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html)：英语语音，109个说话人，每个说话人平均400句话，每句话4至10秒，平行语料。
> - [TIMIT Acoustic-Phonetic Continuous Speech Corpus](https://catalog.ldc.upenn.edu/LDC93S1)：英语语音，630个说话人，每个说话人平均10句话，平行语料。
>
> ### Uncovering Latent Style Factors for Expressive Speech Synthesis
>
> 论文地址：[Uncovering Latent Style Factors for Expressive Speech Synthesis](https://arxiv.org/abs/1711.00520)
>
> 样音：[Audio Samples](https://google.github.io/tacotron/publications/uncovering_latent_style_factors_for_expressive_speech_synthesis/index.html)
>
> 该篇论文是Style Token系列的开篇之作，在Tacotron的基础上实现的风格转换。在原始Tacotron的基础上，添加了如图红框内的`Style Attention`。其中，`Style encoder`由K个`Style token`组成，Style token随机初始化并被全局共享，做无监督学习。训练时，每一帧音频从上图*Output t-1*传入，作为query进入`Text Attention`和`Style Attention`求得两个Context Vector。
>
> ### Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis
>
> 论文地址：[Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis](https://arxiv.org/abs/1803.09017)
>
> 样音：[Audio Samples](https://google.github.io/tacotron/publications/global_style_tokens/index.html)
>
> Tensorflow实现：[cnlinxi/style-token_tacotron2](https://github.com/cnlinxi/style-token_tacotron2)
>
> 该篇是Style Token系列的第二篇，看论文的完成度和样音似乎较为成熟了。该篇文章和上一篇《Uncovering Latent Style Factors for Expressive Speech Synthesis》最大的不同在于：添加Reference encoder，编码一段音频整体的风格特征，该encoder生成的向量，作为Style attention的query求得表征整个音频风格的style embedding。
>
> 提出了一种"global style tokens"(GSTs)模型，该模型包括三个部分：参考编码器(reference encoder)，风格注意力(style attention)，风格编码(style embedding)和序列到序列的生成模型(即Tacotron)。
>
> - Training
>
>   - the reference encoder: 将变长音频的风格压缩到一个固定大小的向量，称此向量为参考嵌入向量(reference embedding)。在训练阶段，参考音频是平行语料中的真实音频。
>   - reference embedding送入注意力模块作为query，K个style token作为key-value，求得Style embedding。其中，style token随机初始化，并在整个训练过程中，被所有音频全局共享。这些style token的集合被称作global style tokens, GSTs。实验中，求得的Style embedding直接与Text encoder的每个编码步拼接即可。
>
> - Inference
>
>   推断阶段有两种模式：一种如上图左侧所示，送入一段想要被模仿风格的音频，深度网络求得该音频的Style embedding进入下游；另一种是手动指定各个style token的权重，注意这里的权重的加和可以不为0，甚至权重可以为负值，同样可以生成Style embedding进入下游。
>
> ### Predicting Expressive Speaking Style From Text In End-To-End Speech Synthesis
>
> 论文地址：[Predicting Expressive Speaking Style From Text In End-To-End Speech Synthesis](https://arxiv.org/abs/1808.01410)
>
> 样音：[Audio Samples](https://google.github.io/tacotron/publications/text_predicting_global_style_tokens/index.html)
>
> 该篇是Style Token系列的第三篇。事实上，这篇论文并不是为了做风格迁移，而是*希望生成更为自然的语音*。在上一篇《Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis》基础上，风格的推断不再是由待模仿音频或手动指定，而是从文本中计算获得。引入一个额外的深度网络，学习原先由Style token layer产生的Style embedding。
>
> ### Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis
>
> 论文地址：[Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis](https://arxiv.org/abs/1806.04558)
>
> 样音：[Audio Samples](https://google.github.io/tacotron/publications/speaker_adaptation/index.html)
>
> 该篇是侧重*多说话人语音合成*的论文，整个模型结构很简单，只是在Tacotron的基础上增加了一个Speaker Encoder的深度网络，用于表征说话人的音色特征。
>
> ### Deep Voice 2: Multi-Speaker Neural Text-to-Speech
>
> 论文地址：[Deep Voice 2: Multi-Speaker Neural Text-to-Speech](https://arxiv.org/abs/1705.08947)
>
> Deep Voice 2是百度提出的，类似于Tacotron的端到端语音合成系统，对该深度网络不是非常熟悉，但是其中也述及多说话人语音合成的问题。
>
> ### Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning
>
> 论文地址：[Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning](https://arxiv.org/abs/1710.07654)
>
> 百度的Deep Voice 2升级版。
>
> ### Neural Voice Cloning with a Few Samples
>
> 论文地址：[Neural Voice Cloning with a Few Samples](https://arxiv.org/abs/1802.06006)
>
> 样音：[Audio Samples](https://audiodemos.github.io/)
>
> 这篇论文主要讨论使用较少的参考语料，就能够模仿一个说话人的声音。
>
> ### Effect of data reduction on sequence-to-sequence neural TTS
>
> 论文地址：[Effect of data reduction on sequence-to-sequence neural TTS](https://arxiv.org/abs/1811.06315)
>
> 亚马逊这篇文章主要探讨了低语料对多说话人语音合成的影响。事实上，机器翻译、语音合成等都受限于平行语料难以获取的问题，有不少文章提出了解决方案，如[Semi-Supervised Training for Improving Data Efficiency in End-to-End Speech Synthesis](https://arxiv.org/abs/1808.10128)，以及[THUNLP-MT/MT-Reading-List](https://github.com/THUNLP-MT/MT-Reading-List)在*Low-resource Language Translation*部分提及的论文。
>
> ### Sample Efficient Adaptive Text-to-Speech
>
> 论文地址：[Sample Efficient Adaptive Text-to-Speech](https://arxiv.org/abs/1809.10460v1)
>
> 端到端语音合成的风格迁移不仅仅可以在频谱生成网络或者频谱上动手脚，当然也可以在声码器上想办法。该篇文章介绍了，在部署时，只需少量数据就能快速适应新说话人的声码器网络。
>
> ### 参考文献：
>
> [Tacotron&Tacotron2——基于深度学习的端到端语音合成模型](https://zhuanlan.zhihu.com/p/101064153)
>
> > 当前最流行的基于深度学习的端到端语音合成模型——Tacotron及其改良版Tacotron2，Tacotron可以仅通过输入(text, wav)数据对儿来直接学习，在经过升级改良之后，最新的Tacotron2的表现已经非常接近人的自然音了；另外，比起以前繁琐的基于pipeline的语音合成模型来说，它不需要手动提取特征，也不需要细致地组合各种模块，你仅仅需要一块高性能的GPU就可以很快得到一个效果接近人类自然音的语音合成模型。
> >
> > 提到Tacotron就不得不提一下Wavenet，实际上自从深度学习流行起来之后，很多基于神经网络来完成语音合成任务的model都涌现了出来，罗马不是一天建成的，Tacotron也不是一开始就有的，它最著名的前辈就是Wavenet。
> >
> > Wavenet并不是一个端到端模型，由于它的输入并不是raw text而是经过处理的特征，因此它实际上只是代替了传统TTS pipeline的后端(回忆我们在[之前文章](https://zhuanlan.zhihu.com/p/99122527)中的概念，传统TTS pipeline由前端和后端组成)。
> >
> > Tacotron是第一个端对端的TTS神经网络模型，输入raw text，Tacotron可以直接输出mel-spectrogram，再利用Griffin-Lim算法就可以生成波形了。
>
> [语音中的风格转换](https://www.cnblogs.com/mengnan/p/10294884.html)
>
> [语音的风格迁移](https://blog.csdn.net/u014479551/article/details/105709627/)
>
> [[语音信号处理] 说话人转换 voice conversion .CycleGANVC .StarGANVC ](https://blog.csdn.net/yyyyyywly/article/details/99304684)
>
> [如何将自己的声音转换成特定的一个人的声音？ - 恐龙贝克APP的回答 - 知乎](https://www.zhihu.com/question/319232781/answer/1340336100)
>
> [Voice Conversion——学习笔记 - 弗拉基米尔的文章 - 知乎 ](https://zhuanlan.zhihu.com/p/134045847)
>
> [语音转换 Voice Conversion - 熬奥凹的文章 - 知乎 ](https://zhuanlan.zhihu.com/p/142877175)



# 2、DEMO

## 环境配置

### Step 1：创建Python虚拟环境

**Python版本要求：**python v3.7.3 or higher

**安装虚拟环境依赖包** : `pip install virtualenv`

**创建Python虚拟环境** : `virtualenv env`

### Step 2：激活Python虚拟环境

**For windows** : `env/Script/activate`

**For Linux** : `source env/bin/activate`

### Step 3：安装Python依赖包

**Install modules** : `pip3 install -r requirements.txt -i https://pypi.douban.com/simple`

**Install pytorch and torchvision** : `pip3 install torch===1.0.0 torchvision===0.2.1 -f https://download.pytorch.org/whl/cu102/torch_stable.html`

> **语音克隆问题总结**
>
> **gcc和g++还是有区别的**
> gcc为C语言的专用编辑器，里面封装有C语言的函数库，虽说也可以用gcc编译C++文件但编译时还要加上C++的函数库才能成功编译，所以建议大家可以下载gcc和g++不同的安装包去编译不同的文件。
>
> **pip install webrtcvad出现错误ERROR: Failed building wheel for webrtcvad**
>
> Use the following command
>
> `pip install webrtcvad-wheels`
>
> **ubuntu OSError: PortAudio library not found**
>
> I could fix this by installing the portaudio library.
>
> ```py
> sudo apt-get install libportaudio2
> ```
>
> You may also try following if this doesn't help.
>
> ```py
> sudo apt-get install libasound-dev
> ```
>
> 





### Step 4：运行Demo

**Run command : **`python3 demo.py  --config config/vox-adv-256.yaml --driving_video 112.mp4 --source_image s.jpg --checkpoint vox-adv-cpk.pth.tar --relative --adapt_scale`

**其他辅助指令**

**Crop-Video : **`python crop-video.py --inp 112.mp4`

# 3、原理

## 3.1、框架

## 3.2、结构

## 3.3、代码

# 4、应用

# 5、附录

## 5.1、多版本Python共存

如何在ubuntu下面使用多个Python和Pip版本？关键命令：**update-alternatives**

> update-alternatives命令用来维护系统命令的符号链接，可以将多个文件链接到同一个符号文件上，并进行管理。

### 5.1.1、查看Python版本号：

```
$ python -V or (python --version)
$ python2 -V
$ python3 -V
```

### 5.1.2、查看Python不同版本对应的路径：

```bash
$ which python3
/usr/bin/python3.6
$ which python3.7
/usr/local/bin/python3.7
```

### 5.1.3、使用**update-alternatives --install**建立链接

```bash
$ sudo update-alternatives --install /usr/bin/python python /usr/bin/python3.6 1
$ sudo update-alternatives --install /usr/bin/python python /usr/local/bin/python3.7 2
```

> 即我们在/usr/bin/python这个目录下，建立一个链接符号为“Python”的链接，这里指定了两个目录，分别是Python3.6和Python3.7的。后面还有一个数字，分别是1和2，代表了优先级，**数字越大优先级越高**，这里明显默认选择python3.7！
>
> 查看已经添加的python:
>
> ```shell
> $ sudo update-alternatives --list python
> ```

### 5.1.4、选择要执行的Python版本

```bash
$ sudo update-alternatives --config python
```

### 5.1.5、pip设置方法与此类似

```
$ pip -V or (python --version)
$ pip2 -V
$ pip3 -V
$ which pip3
$ which pip3.7
$ sudo update-alternatives --install /usr/bin/pip pip /usr/bin/pip3 1
$ sudo update-alternatives --install /usr/bin/pip pip /usr/local/bin/pip3.7 2
$ sudo update-alternatives --config pip
```

### 5.1.6【参考文献】

[在Ubuntu18.04下存在多版本Python如何设置默认Python和Pip？](https://blog.csdn.net/nightelf00/article/details/89408658)

[ubuntu安装多版本python](https://www.cnblogs.com/baicailong/p/9751019.html)

[Ubuntu软链接管理神器update-alternatives](https://www.jianshu.com/p/7744336b5bcb)

## 5.2、Python虚拟环境

**virtualenv**是用来建立一个虚拟的python环境，一个专属于项目的python环境，对于想要保持一个干净的Python依赖包管理环境的人非常有用。

### 5.2.1、安装virtualenv

通过pip安装virtualenv：

```
pip install virtualenv
```

测试安装:

```
virtualenv --version
```

### 5.2.2、创建一个Python虚拟环境

为一个工程项目搭建一个虚拟环境:

```
cd my_project
virtualenv my_project_env
```

> 另外，如果存在多个python解释器，可以选择指定一个Python解释器（比如``python2.7``），没有指定则由系统默认的解释器来搭建： 
>
> ```
> virtualenv -p /usr/bin/python2.7 my_project_env
> ```
>
> 将会在当前的目录中创建一个名my_project_env的文件夹，这是一个独立的python运行环境，包含了Python可执行文件， 以及 `pip` 库的一份拷贝，这样就能安装其他包了，不过已经安装到系统Python环境中的所有第三方包都不会复制过来，这样，我们就得到了一个不带任何第三方包的“干净”的Python运行环境来。

### 5.2.3、Python虚拟环境的使用

使用虚拟环境之前需要被激活：

```
source my_project_env/bin/activate
```

停用虚拟环境直接输入：

```
deactivate
```

停用后将回到系统默认的Python解释器

### 5.2.4、【参考文献】

[python三大神器之virtualenv](https://www.cnblogs.com/freely/p/8022923.html)

[virtualenv · 廖雪峰的Python3.x教程 · 看云](https://www.kancloud.cn/smilesb101/python3_x/298883)

## 5.3、Docker&Nvidia-Docker

### 5.3.1、Docker

**1.更换国内软件源，推荐中国科技大学的源，稳定速度快（可选）**

```
sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak
sudo sed -i 's/archive.ubuntu.com/mirrors.ustc.edu.cn/g' /etc/apt/sources.list
sudo apt update
```

**2.安装需要的包**

```
sudo apt install apt-transport-https ca-certificates software-properties-common curl
```

**3.添加 GPG 密钥，并添加 Docker-ce 软件源，这里还是以中国科技大学的 Docker-ce 源为例**

```
curl -fsSL https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository "deb [arch=amd64] https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu \
$(lsb_release -cs) stable"
```

**4.添加成功后更新软件包缓存**

```
sudo apt update
```

**5.安装 Docker-ce**

```
sudo apt install docker-ce
```

**6.设置开机自启动并启动 Docker-ce（安装成功后默认已设置并启动，可忽略）**

```
sudo systemctl enable docker
sudo systemctl start docker
```

**7.测试运行**

```
sudo docker run hello-world
```

**8.添加当前用户到 docker 用户组，可以不用 sudo 运行 docker（可选）**

```
sudo groupadd docker
sudo usermod -aG docker $USER
```

**9.测试添加用户组（可选）**

```
docker run hello-world
```

### 5.3.2、Nvidia-Docker

**1. 卸载 nvidia-docker 1.0 及其他GPU容器**

```
docker volume ls -q -f driver=nvidia-docker | xargs -r -I{} -n1 docker ps -q -a -f volume={} | xargs -r docker rm -f
sudo apt-get purge -y nvidia-docker
```

**2. 添加package repositories**

```
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | \
  sudo apt-key add -
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
  sudo tee /etc/apt/sources.list.d/nvidia-docker.list
sudo apt-get update
```

**3. 安装 nvidia-docker2**

```
sudo apt-get install -y nvidia-docker2
sudo pkill -SIGHUP dockerd
```

**4. 测试安装**

```
docker run --runtime=nvidia --rm nvidia/cuda:9.0-base nvidia-smi
```

### 5.3.3、【参考文献】

[Ubuntu16.04安装Docker、nvidia-docker](https://www.cnblogs.com/leton/p/11674796.html)

[Ubuntu18.04安装nvidia-docker（亲测有效，步骤详尽）](https://blog.csdn.net/BigData_Mining/article/details/99681168)

## 5.4、FFmpeg使用指南

**FFmpeg官网：**http://ffmpeg.org/

**安装ffmpeg：**

http://www.cnblogs.com/freeweb/p/6897907.html

### 5.4.1、参数解释

**主要参数：**

-i 设定输入流

-f 设定输出格式

-ss 开始时间

**视频参数：**

-b 设定视频流量，默认为200Kbit/s

-r 设定帧速率，默认为25

-s 设定画面的宽与高

-aspect 设定画面的比例

-vn 不处理视频

-vcodec 设定视频编解码器，未设定时则使用与输入流相同的编解码器

**音频参数：**

-ar 设定采样率

-ac 设定声音的Channel数

-acodec 设定声音编解码器，未设定时则使用与输入流相同的编解码器

-an 不处理音频

**拓展：**

-strict -2 之前是实验参数表示aac音频编码，如果不使用aac音频编码使用使其他的编码，好像还需要导入第三方的音频编码库，比较麻烦。使用FFmpeg自带的aac音频编码要带上-strict -2 参数就可以了。带这个参数是为了使用aac音频编码。

-sameq 指相同的量级数，常被误用成“相同的画质”，所以被取消。官方推荐用-qscale和-qmax替代 ffmpeg不支持libfaac库，faac库音质也一般。 

另支持2套AAC音频编码， -acodec aac 是ffmpeg自制的AAC编码器，处在实验阶段，需要开启"-strict experimental"参数，音质也不差。 -acodec libvo_aacenc 是移植自安卓系统的AAC编码器，比较成熟。

### 5.4.2、常用指令

- **mp3转wav**

```
$ ffmpeg -i input.mp3 -acodec pcm_s16le -ac 1 -ar 16000 output.wav
$ ffmpeg -i y.mp3 -ss 00:00:01 -t 4.5 -acodec pcm_s16le -ac 1 -ar 24000 y.wav
```

- **m4a转wav**

```
$ ffmpeg.exe -i input.m4a -ac 2 -ar 44100 -acodec pcm_s16le -f wav output.wav
```

- **mov转mp4**

```
$ ffmpeg -i source.mov -y -c:v copy source.mp4
$ ffmpeg -i 000.MOV -ss 0.5 -t 4.5 -filter:v "crop=650:650:20:80, scale=256:256" 000.mp4
$ ffmpeg -i 000.MOV -strict -2 -vf crop=600:600:20:100 out.mp4
```

- **从视频中提取声音**

```
$ ffmpeg -i [input].mp4 -vn -ab 128k [output].mp3
```

- **分离视频音频流**

```
//分离视频流
$ ffmpeg -i input_file -vcodec copy -an output_file_video
//分离音频流
$ ffmpeg -i input_file -acodec copy -vn output_file_audio
```

- **去掉视频里的声音（静音）**

```
$ ffmpeg -i [input].mp4 -an [output].mp4
```

- **改变视频文件大小（分辨率）**

```
$ ffmpeg -i [input].mp4 -s 640x480 -c:a copy [output].mp4
```

- **截取一段音频**

```
//-ss:截取开始时间点， -t：要截取的视频长度（15秒）
$ ffmpeg -ss 00:00:15 -t 45 -i sampleaudio.mp3 croppedaudio.mp3
$ ffmpeg -i [input].mp4 -ss 00:00:00 -codec copy -t 15 [output].mp4
```

- **视频时间剪切**

```
//-r 提取图像的频率，-ss 开始时间，-t 持续时间
$ ffmpeg –i test.avi –r 1 –f image2 image-%3d.jpeg        //提取图片
$ ffmpeg -ss 0:1:30 -t 0:0:20 -i input.avi -vcodec copy -acodec copy output.avi    //剪切视频
```

- **视频空间剪切**

```
$ ffmpeg -i input.mp4 -filter:v "crop=w:h:x:y" output.mp4

input.mp4 – 源视频文件。
-filter:v – 表示视频过滤器。
crop – 表示裁剪过滤器。
w – 我们想自源视频中裁剪的矩形的宽度。
h – 矩形的高度。
x – 我们想自源视频中裁剪的矩形的 x 坐标 。
y – 矩形的 y 坐标。

比如说你想要一个来自视频的位置 (200,150)，且具有 640 像素宽度和 480 像素高度的视频，命令应该是：

$ ffmpeg -i input.mp4 -filter:v "crop=640:480:200:150" output.mp4
```

- **把一个视频分成多个部分**

```
//0-59秒一部分，59秒以后一部分
$ ffmpeg -i input.mp4 -t 00:00:59 -c copy part1.mp4 -ss 00:00:59 -codec copy part2.mp4
```

- **查看ffmpeg支持的视频格式**

```
$ ffmpeg -formats
```

- **mp4到wmv格式转换**

```
$ ffmpeg -i input.mp4 -c:v libx264 output.wmv
```

- **webm转为mp4**

```
$ ffmpeg -i input.webm -qscale 0 output.mp4
```

- **视频文件名写入txt**

```
$ ffmpeg -i input.webm -qscale 0 output.mp4
```

- **对音频加减速**

```
$ ffmpeg -i input.mkv -filter:a "atempo=2.0" -vn output.mkv
```

- **对视频加减速**

```
$ ffmpeg -i input.mp4 -filter:v "setpts=0.125*PTS" output.mp4
```

-  **旋转视频**

```
$ ffmpeg -i input.mp4 -filter:v 'transpose=1' rotated-video.mp4
$ ffmpeg -i input.mp4 -filter:v 'transpose=2,transpose=2' rotated-video.mp4
```

- **改变声音大小**

```
$ ffmpeg -i input.wav -af 'volume=0.5' output.wav
```

- **加入字幕**

```
$ ffmpeg -i movie.mp4 -i subtitles.srt -map 0 -map 1 -c copy -c:v libx264 -crf 23 -preset veryfast output.mkv
```

- **把单独的一个图片转为视频**

```
$ ffmpeg -loop 1 -i image.png -c:v libx264 -t 30 -pix_fmt yuv420p video.mp4
```

- **把视频文件转为图片**

```
$ ffmpeg -i movie.mp4 -r 0.25 frames_%04d.png
```

- **视频中提取帧**

```
$ ffmpeg -ss 00:00:15 -i video.mp4 -vf scale=800:-1 -vframes 1 image.jpg
```

- **把视频转为GIF动态图**

```
$ ffmpeg -i video.mp4 -vf scale=500:-1 -t 10 -r 10 image.gif
```

- **左右声道的录音合成为立体声**

```
$ ffmpeg -i 1.wav -i 2.wav -filter_complex "amovie=1.wav [l]; amovie=2.wav [r]; [l] [r] amerge" 1_2.mp3
```

- **从视频里截图**

```
$ ffmpeg -i test.avi -y -f image2 -ss 8 -t 0.001 -s 350x240 test.jpg
```

- **音视频文件的切割**

```
$ ffmpeg -ss 00:00:10 -t 00:01:22 -i input.mp3 output.mp3
```

- **视频解复用**

```
$ ffmpeg –i test.mp4 –vcodec copy –an –f m4v test.264
$ ffmpeg –i test.avi –vcodec copy –an –f m4v test.264
```

- **视频转码**

```
//-bf B帧数目控制，-g 关键帧间隔控制，-s 分辨率控制

//转码为码流原始文件
$ ffmpeg –i test.mp4 –vcodec h264 –s 352*278 –an –f m4v test.264              
//转码为码流原始文件
$ ffmpeg –i test.mp4 –vcodec h264 –bf 0 –g 25 –s 352*278 –an –f m4v test.264
//转码为封装文件
$ ffmpeg –i test.avi -vcodec mpeg4 –vtag xvid –qsame test_xvid.avi
```

- **视频封装**

```
$ ffmpeg –i video_file –i audio_file –vcodec copy –acodec copy output_file
```

### 5.4.3、【参考文献】

[关于FFmpeg工具的使用总结](https://www.cnblogs.com/runchen0518/p/7895948.html)

[给新手的 20 多个 FFmpeg 命令示例](https://zhuanlan.zhihu.com/p/67878761)

## 5.5、相关问题与解答

### 5.5.1、Ubuntu 18.04开放指定端口

一般情况下，ubuntu安装好的时候，iptables会被安装上，如果没有的话先安装
`sudo apt-get install iptables`

添加开放端口

```shell
# sudo iptables -I INPUT -p tcp --dport [端口号] -j ACCEPT
sudo iptables -I INPUT -p tcp --dport 80 -j ACCEPT
# 临时保存配置，重启后失效
sudo iptables-save
1234
```

安装 iptables-persistent工具，持久化开放端口配置
`sudo apt-get install iptables-persistent`

```shell
sudo netfilter-persistent save
sudo netfilter-persistent reload
```

### 5.5.2、flask permission denied解决方案

端口号不在合法范围内，端口号应该在1024~65535之间

### 5.5.3、ffmpeg多个视频为一个视频

【[参考文献：使用FFmpeg合并音视频](https://www.jianshu.com/p/2a824f13b2af)】

**横向合并视频**

ffmpeg -i input1.mp4 -i input2.mp4 -lavfi hstack output.mp4

上面的命令虽然可以合并视频，两个视频可以正常播放，但是只保留了前面一个的音频。
下面会介绍怎么避开这个坑。

注意这时候input1和input2必须同样的高度，如果不一样的高度可以使用-shortest参数来保证同样的高度。

如果希望合并多个视频，可以使用下面命令行。

```
ffmpeg -i input1.mp4 -i input2.mp4 -i input3.mp4 -lavfi hstack=inputs=3 output.mp4

其中input=3表示希望合并的视频的个数
```

**纵向合并视频**

```
ffmpeg -i input1.mp4 -i input2.mp4 -lavfi vstack output.mp4
```

### 5.5.4、ffmpeg混合多个音频为一个音频

ffmpeg依赖yasm,首先安装yasm：sudo apt-get install yasm；其次，sudo apt-get install ffmpeg。

```
ffmpeg -y  -i input1.wav  -iinput2.wav -filter_complex amix=inputs=2:duration=first:dropout_transition=4  output.wav

input1.wav 和 input2.wav是需要混合输入的两个音频；

amix=inputs=输入的音频个数，默认是2；

duration=音频周期时常，默认是最长的；
```

